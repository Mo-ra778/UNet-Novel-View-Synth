<div align="center">

# ğŸ¯ U-Net Novel View Synthesis

### *Generate New 3D Viewpoints from Single Images Using Deep Learning*

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Gradio](https://img.shields.io/badge/Demo-Gradio-orange.svg)](https://gradio.app/)

<p align="center">
  <img src="https://img.shields.io/badge/Status-Active-success?style=for-the-badge" alt="Status"/>
  <img src="https://img.shields.io/badge/GPU-CUDA-76B900?style=for-the-badge&logo=nvidia" alt="CUDA"/>
</p>

---

**A deep learning project that learns to synthesize novel viewpoints of 3D objects from a single input image, leveraging the power of U-Net architecture with perceptual loss optimization.**

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Architecture](#-architecture)
- [Installation](#-installation)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [Training](#-training)
- [Web Demo](#-web-demo)
- [Results](#-results)
- [Configuration](#-configuration)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸ” Overview

Novel View Synthesis (NVS) is a fundamental problem in computer vision and graphics. This project implements a **U-Net based encoder-decoder architecture** that learns to generate new viewing angles of 3D geometric shapes from a single input image.

### Key Highlights:
- ğŸ¨ **9 Geometric Shapes**: Cube, Sphere, Cylinder, Cone, Pyramid, Torus, Octahedron, Dodecahedron, Icosahedron
- ğŸ“ **Fibonacci Sphere Sampling**: Optimal uniform distribution of camera viewpoints
- ğŸš€ **Real-time Inference**: Fast prediction with GPU acceleration
- ğŸŒ **Interactive Web Demo**: Gradio-powered interface for easy testing

---

## âœ¨ Features

| Feature                      | Description                                                          |
| ---------------------------- | -------------------------------------------------------------------- |
| **U-Net Architecture**       | Encoder-decoder with skip connections for preserving spatial details |
| **Perceptual Loss**          | VGG-based feature matching for high-quality image generation         |
| **Mixed Precision Training** | FP16 training for faster computation and reduced memory              |
| **Auto Resume**              | Automatic checkpoint saving and training resumption                  |
| **Live Dashboard**           | Real-time training metrics visualization                             |
| **Web Interface**            | Interactive Gradio demo for testing predictions                      |
| **Fibonacci Sampling**       | Mathematically optimal viewpoint distribution                        |

---

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    U-Net Architecture                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   Input Image (256Ã—256Ã—3) + Target Camera (3)               â”‚
â”‚                        â†“                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚              ENCODER (Contracting Path)              â”‚   â”‚
â”‚   â”‚  Convâ†’BNâ†’ReLUâ†’Convâ†’BNâ†’ReLUâ†’MaxPool (Ã—4 blocks)      â”‚   â”‚
â”‚   â”‚  64 â†’ 128 â†’ 256 â†’ 512 channels                       â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                        â†“                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                   BOTTLENECK                         â”‚   â”‚
â”‚   â”‚              1024 channels (16Ã—16)                   â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                        â†“                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚              DECODER (Expanding Path)                â”‚   â”‚
â”‚   â”‚  UpConvâ†’Concatâ†’Convâ†’BNâ†’ReLU (Ã—4 blocks)             â”‚   â”‚
â”‚   â”‚  512 â†’ 256 â†’ 128 â†’ 64 channels                       â”‚   â”‚
â”‚   â”‚  + Skip Connections from Encoder                     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                        â†“                                     â”‚
â”‚              Output Image (256Ã—256Ã—3)                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Installation

### Prerequisites

- Python 3.8 or higher
- CUDA-capable GPU (recommended)
- Git

### Step-by-Step Setup

```bash
# Clone the repository
git clone https://github.com/Mo-ra778/UNet-Novel-View-Synth.git
cd UNet-Novel-View-Synth

# Create virtual environment (recommended)
python -m venv venv

# Activate virtual environment
# On Windows:
venv\Scripts\activate
# On Linux/Mac:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Dependencies

```txt
torch>=2.0.0
torchvision>=0.15.0
numpy>=1.21.0
Pillow>=9.0.0
gradio>=3.0.0
matplotlib>=3.5.0
tqdm>=4.64.0
```

---

## ğŸš€ Usage

### Quick Start - Web Demo

Launch the interactive web demo to test the model:

```bash
cd "Phase4_UNet_Pro - Ù†Ø³Ø®Ø©/Phase4_UNet_Pro"
python web_demo.py
```

Then open your browser at `http://localhost:7860`

### Command Line Testing

```bash
python test_model.py --input path/to/image.png --elevation 45 --azimuth 90
```

### Full Evaluation

```bash
python full_evaluation.py
```

---

## ğŸ“ Project Structure

```
UNet-Novel-View-Synth/
â”‚
â”œâ”€â”€ ğŸ“‚ Phase4_UNet_Pro - Ù†Ø³Ø®Ø©/
â”‚   â””â”€â”€ ğŸ“‚ Phase4_UNet_Pro/
â”‚       â”‚
â”‚       â”œâ”€â”€ ğŸ§  model_unet.py          # U-Net architecture definition
â”‚       â”œâ”€â”€ ğŸ“Š dataloader_phase4.py   # Data loading and preprocessing
â”‚       â”œâ”€â”€ ğŸ¯ loss_perceptual.py     # Perceptual loss implementation
â”‚       â”œâ”€â”€ ğŸ‹ï¸ train_phase4_unet.py   # Training script
â”‚       â”œâ”€â”€ ğŸ§ª test_model.py          # Model testing utility
â”‚       â”œâ”€â”€ ğŸ“ˆ full_evaluation.py     # Comprehensive evaluation
â”‚       â”œâ”€â”€ ğŸŒ web_demo.py            # Gradio web interface
â”‚       â”œâ”€â”€ ğŸ”§ benchmark.py           # Performance benchmarking
â”‚       â”‚
â”‚       â”œâ”€â”€ ğŸ“‚ checkpoints/           # Saved model weights
â”‚       â”œâ”€â”€ ğŸ“‚ training_samples/      # Training visualizations
â”‚       â””â”€â”€ ğŸ“‚ evaluation_results/    # Evaluation outputs
â”‚
â”œâ”€â”€ ğŸ“„ .gitignore                     # Git ignore rules
â”œâ”€â”€ ğŸ“„ README.md                      # This file
â””â”€â”€ ğŸ“„ requirements.txt               # Python dependencies
```

---

## ğŸ‹ï¸ Training

### Start Training from Scratch

```bash
python train_phase4_unet.py
```

### Training Configuration

Edit the training script to customize:

| Parameter       | Default | Description                   |
| --------------- | ------- | ----------------------------- |
| `batch_size`    | 8       | Batch size for training       |
| `learning_rate` | 1e-4    | Initial learning rate         |
| `num_epochs`    | 100     | Total training epochs         |
| `image_size`    | 256     | Input/output image resolution |
| `num_shapes`    | 9       | Number of geometric shapes    |

### Training Features

- âœ… **Automatic Checkpointing**: Saves best model based on validation loss
- âœ… **Learning Rate Scheduling**: Reduces LR on plateau
- âœ… **Mixed Precision (FP16)**: Faster training with reduced memory
- âœ… **Live Dashboard**: Real-time loss and PSNR visualization
- âœ… **Resume Capability**: Automatically resumes from last checkpoint

---

## ğŸŒ Web Demo

The project includes an interactive **Gradio** web interface:

### Features:
- ğŸ–¼ï¸ **Image Upload**: Upload source images or select from examples
- ğŸšï¸ **Camera Controls**: Adjust elevation (0-90Â°) and azimuth (0-360Â°)
- âš¡ **Real-time Prediction**: Instant novel view generation
- ğŸ“Š **Quality Metrics**: Display PSNR and inference time

### Launch Demo:

```bash
python web_demo.py
```

Access at: `http://localhost:7860`

---

## ğŸ“Š Results

### Performance Metrics

| Metric             | Value       |
| ------------------ | ----------- |
| **PSNR**           | ~25-30 dB   |
| **SSIM**           | ~0.85-0.92  |
| **Inference Time** | ~50ms (GPU) |
| **Model Size**     | ~124 MB     |

### Supported Shapes

| Shape       | Preview | Shape        | Preview |
| ----------- | ------- | ------------ | ------- |
| Cube        | ğŸŸ¦       | Sphere       | ğŸ”µ       |
| Cylinder    | ğŸ”·       | Cone         | ğŸ”º       |
| Pyramid     | ğŸ”»       | Torus        | ğŸ©       |
| Octahedron  | ğŸ’       | Dodecahedron | â¬¡       |
| Icosahedron | âš½       |              |         |

---

## âš™ï¸ Configuration

### Camera Distribution

The project uses **Fibonacci Sphere Sampling** for optimal camera placement:

```
Fibonacci Spiral Distribution:
â€¢ 40 viewpoints per shape
â€¢ Uniform coverage of viewing hemisphere  
â€¢ Elevation: 0Â° to 90Â°
â€¢ Azimuth: 0Â° to 360Â°
```

This ensures:
- âœ… Maximum diversity in training data
- âœ… No clustering at poles
- âœ… Mathematically optimal distribution

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/AmazingFeature`)
3. **Commit** your changes (`git commit -m 'Add AmazingFeature'`)
4. **Push** to the branch (`git push origin feature/AmazingFeature`)
5. **Open** a Pull Request

### Areas for Contribution:
- ğŸ¨ Add more 3D shapes
- ğŸš€ Optimize inference speed
- ğŸ“± Mobile-friendly demo
- ğŸ“š Improve documentation
- ğŸ§ª Add more test cases

---

## ğŸ“œ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¨â€ğŸ’» Author

**Mo-ra778**

- GitHub: [@Mo-ra778](https://github.com/Mo-ra778)

---

## ğŸ™ Acknowledgments

- PyTorch team for the amazing deep learning framework
- Gradio for the easy-to-use web interface library
- The computer vision research community

---

<div align="center">

### â­ Star this repo if you find it useful!

**Made with â¤ï¸ and PyTorch**

</div>
